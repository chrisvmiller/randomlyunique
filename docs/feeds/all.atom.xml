<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>randomlyunique</title><link href="http://www.randomlyunique.com/" rel="alternate"></link><link href="http://www.randomlyunique.com/feeds/all.atom.xml" rel="self"></link><id>http://www.randomlyunique.com/</id><updated>2017-03-17T00:00:00-07:00</updated><entry><title>Glib Graphology</title><link href="http://www.randomlyunique.com/data/2017/glib-graphology/" rel="alternate"></link><updated>2017-03-17T00:00:00-07:00</updated><author><name></name></author><id>tag:www.randomlyunique.com,2017-03-17:data/2017/glib-graphology/</id><summary type="html">&lt;p&gt;Graphology, the act of evaluating personality characteristics from handwriting, isn't 
too objective, but - hey - I'll still take a crack at it!  Using a set of 
300 famous signatures, I'll attempt to compose my personality from various celebrities! &lt;/p&gt;
&lt;p&gt;The algorithm in a nutshell involved calculating image keypoints via a Scale-Invariant 
Feature Transform (SIFT), followed by a simple pixel-wise Manhattan distance comparison 
between my keypoints (it's actually a small normalized neighbourhood around each keypoint) and each celeb. &lt;br /&gt;
The chart below is my purposed celebrity mixture: &lt;/p&gt;
&lt;p&gt;&lt;img alt="Photo" class="image_center_style" src="http://www.randomlyunique.com/data/2017/glib-graphology/glib-graphology.png" /&gt;&lt;/p&gt;</summary></entry><entry><title>What Makes a Clock Tick</title><link href="http://www.randomlyunique.com/plancky/2017/what-makes-a-clock-tick/" rel="alternate"></link><updated>2017-02-19T00:00:00-08:00</updated><author><name></name></author><id>tag:www.randomlyunique.com,2017-02-19:plancky/2017/what-makes-a-clock-tick/</id><summary type="html">&lt;p&gt;&lt;img alt="Photo" class="image_center_style" src="http://www.randomlyunique.com/plancky/2017/what-makes-a-clock-tick/what-makes-a-clock-tick.png" /&gt;&lt;/p&gt;</summary></entry><entry><title>How a Laser Works</title><link href="http://www.randomlyunique.com/plancky/2017/how-a-laser-works/" rel="alternate"></link><updated>2017-01-22T00:00:00-08:00</updated><author><name></name></author><id>tag:www.randomlyunique.com,2017-01-22:plancky/2017/how-a-laser-works/</id><summary type="html">&lt;p&gt;&lt;img alt="Photo" class="image_center_style" src="http://www.randomlyunique.com/plancky/2017/how-a-laser-works/how-a-laser-works.png" /&gt;&lt;/p&gt;</summary></entry><entry><title>Why the Sky is Blue</title><link href="http://www.randomlyunique.com/plancky/2016/sky-is-blue/" rel="alternate"></link><updated>2016-12-15T00:00:00-08:00</updated><author><name></name></author><id>tag:www.randomlyunique.com,2016-12-15:plancky/2016/sky-is-blue/</id><summary type="html">&lt;p&gt;&lt;img alt="Photo" class="image_center_style" src="http://www.randomlyunique.com/plancky/2016/sky-is-blue/why-the-sky-is-blue.png" /&gt;&lt;/p&gt;</summary></entry><entry><title>Bay Area Heatmap</title><link href="http://www.randomlyunique.com/data/2016/bay-area-heatmap/" rel="alternate"></link><updated>2016-11-30T00:00:00-08:00</updated><author><name></name></author><id>tag:www.randomlyunique.com,2016-11-30:data/2016/bay-area-heatmap/</id><summary type="html">&lt;p&gt;After a few years in the Bay Area, my wife (she's lived in SF for over 20 years!)
and I decided to move to the almost literal greener pasture of Seattle. While sitting 
in my circa 1998 SF-priced downtown Seattle apartment (with a frigging washer/dryer and dishwasher),
I decided to aggregate my phones location data. The heatmap below filters out where I lived 
and worked, while plotting the logarithm of my phones recorded location count:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Photo" class="image_center_style" src="http://www.randomlyunique.com/data/2016/bay-area-heatmap/bay-area-heatmap.png" /&gt;&lt;/p&gt;
&lt;p&gt;Cool observations: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can clearly see my morning Embarcadero coffee walk into work&lt;/li&gt;
&lt;li&gt;This common conversation: "ok, it's Saturday night at 10pm - where should we go?" "Umm, pretty sure the Mission is the only place open at those late hours"&lt;/li&gt;
&lt;li&gt;My incredibly beloved BART ride to where I lived in Berkeley (I can still hear BARTs melodic banshee wail in my dreams) &lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>Reinforcement Sort</title><link href="http://www.randomlyunique.com/data/2016/reinforcement-sort/" rel="alternate"></link><updated>2016-09-15T00:00:00-07:00</updated><author><name></name></author><id>tag:www.randomlyunique.com,2016-09-15:data/2016/reinforcement-sort/</id><summary type="html">&lt;p align="center"&gt;
You know Quicksort and Merge sort and Timsort and Shell sort &lt;br&gt;
Heapsort and Bubble sort and Insertion sort and Selection sort &lt;br&gt;
But do you recall the most god-awful sort of all? 
&lt;/p&gt;

&lt;p&gt;A Reinforcement Sort (a sort done through reinforcement(ish) learning) is the Nickelback of sorting 
algorithms - a perfect superposition of horrible and awesome. This rockstar was developed using an 
epsilon greedy &lt;a href=https://en.wikipedia.org/wiki/Q-learning&gt;Q-learning&lt;/a&gt; approach
with the aim of generating the best python sorting script. This mainly involved combining code 
fragments together and giving rewards based on how well these scripts sort a list. The exact setup is 
summarized as follows:&lt;/p&gt;
&lt;p&gt;1) &lt;a href=https://github.com/chrisvmiller/analytics/blob/master/reinforcement_sort/action.py#L50&gt;State&lt;/a&gt;: A binary array chad-kroegered from a md5 hash of the python code string &lt;br&gt;
2) &lt;a href=https://github.com/chrisvmiller/analytics/blob/master/reinforcement_sort/action.py#L8&gt;Actions&lt;/a&gt;: 
I figured sorting will require some looping, swapping and probably a conditional or two, so I made these actions on a state&lt;br&gt;
3) &lt;a href=https://github.com/chrisvmiller/analytics/blob/master/reinforcement_sort/reward.py&gt;Reward&lt;/a&gt;: 
The state receives a score of -1 if the python code doesn't run, -10 is the code is over 300 characters or +1000 if the list is correctly sorted. &lt;br&gt;&lt;/p&gt;
&lt;p&gt;After &lt;a href=https://github.com/chrisvmiller/analytics/blob/master/reinforcement_sort/learn_sort.py&gt;training&lt;/a&gt; up a neural net, 
I received this optimal sort: &lt;/p&gt;
&lt;p&gt;&lt;img alt="Photo" class="image_center_style" src="http://www.randomlyunique.com/data/2016/reinforcement-sort/reinforcement-sort.png" /&gt;&lt;/p&gt;
&lt;p&gt;Now you may be telling yourself that this looks exactly like a Bubble Sort, but I assure you it's not. 
A Bubble Sort has a reasonable time complexity of O(n^2), but a Reinforcement Sort 
runs in O(oh god, why am I doing this).&lt;/p&gt;</summary></entry><entry><title>A Token of Tolkien</title><link href="http://www.randomlyunique.com/data/2016/a-token-of-tolkien/" rel="alternate"></link><updated>2016-07-03T00:00:00-07:00</updated><author><name></name></author><id>tag:www.randomlyunique.com,2016-07-03:data/2016/a-token-of-tolkien/</id><summary type="html">&lt;p&gt;The Hobbit and The Lord of the Rings are undeniably great books, with rich complex characters and a deep thoughtful story. 
So, this seems like an ideal case to apply an autosummary algorithm meant for tasks like factual news articles!&lt;/p&gt;
&lt;p&gt;The &lt;a href=https://github.com/chrisvmiller/analytics/blob/master/autosummarizer/summarize.py&gt;method&lt;/a&gt; I programmed is pretty straightforward,
where I find the most representative sentence within a given blob of text. This is accomplished by tokenizing each sentence, converting to a 
tf-idf matrix, multiplying this matrix by it's transpose to get a similarity between sentences, mapping to a graph where each node is a sentence
and each edge is its similarity, then PageRanking to determine the 'best' sentence. &lt;/p&gt;
&lt;p&gt;The 'ok, these seems reasonable, I guess' results are shown below:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Photo" class="image_center_style" src="http://www.randomlyunique.com/data/2016/a-token-of-tolkien/a-token-of-tolkien_part1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="color:green; font-weight: bold;"&gt;Special Bonus:&lt;/span&gt; When I apply this autosummary to sentences that contain the word "Sam" throughout The Lord of the Rings trilogy, I get: &lt;/p&gt;
&lt;p&gt;&lt;img alt="Photo" class="image_center_style" src="http://www.randomlyunique.com/data/2016/a-token-of-tolkien/a-token-of-tolkien_part2.png" /&gt;&lt;/p&gt;</summary></entry><entry><title>Hasta la Palindrome</title><link href="http://www.randomlyunique.com/data/2016/hasta-la-palindrome/" rel="alternate"></link><updated>2016-05-02T00:00:00-07:00</updated><author><name></name></author><id>tag:www.randomlyunique.com,2016-05-02:data/2016/hasta-la-palindrome/</id><summary type="html">&lt;p&gt;As a tech worker, I can unequivocally state that the only thing more important than debating coding 
styles or following Agile methodologies is the ability to rapidly whiteboard algorithms. So, if
you're ever asked to write a palindrome detector during a tech screen, be sure to impress 
the interviewer with this beauty, written in &lt;a href=http://lhartikk.github.io/ArnoldC/&gt;ArnoldC&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Photo" class="image_center_style" src="http://www.randomlyunique.com/data/2016/hasta-la-palindrome/hasta-la-palindrome.png" /&gt;&lt;/p&gt;</summary></entry><entry><title>Sommelier Training</title><link href="http://www.randomlyunique.com/data/2016/sommelier-training/" rel="alternate"></link><updated>2016-03-10T00:00:00-08:00</updated><author><name></name></author><id>tag:www.randomlyunique.com,2016-03-10:data/2016/sommelier-training/</id><summary type="html">&lt;p&gt;What's the best way to refine my wine intelligence? Since nothing immediately comes to mind, I'll take the fallback 
algorithmic approach and brute-force a solution: sampling lots and lots of wine! So, let's determine the amount 
of wine I need to drink to achieve the same culinary genius as Robin Williams in Neverland!&lt;/p&gt;
&lt;p&gt;After googling around for a few minutes, I was able to find a wine training set composed of a dozen wine features 
(acidity, pH, alcohol, etc) and a ranked quality. I then proceeded to classify this wine quality (at different splits: good/bad, good/okay/bad, etc) 
with a fancy-pants &lt;a href=https://github.com/chrisvmiller/analytics/blob/master/sommelier_training/WineClassifier.ipynb&gt;neural network&lt;/a&gt;. 
The converged prediction iteration count, after normalizing by the amount of wine it takes to blindfoldedly distinguish 
between red and white, is shown in the chart below: &lt;/p&gt;
&lt;p&gt;&lt;img alt="Photo" class="image_center_style" src="http://www.randomlyunique.com/data/2016/sommelier-training/sommelier-training.png" /&gt;&lt;/p&gt;
&lt;p&gt;Now, I only need to convince my wife that purchasing 7 Balthazars of pink bubbly wine is intended for research 
and my life will be fantastic.&lt;/p&gt;</summary></entry><entry><title>Late Night Coding</title><link href="http://www.randomlyunique.com/data/2016/late-night-coding/" rel="alternate"></link><updated>2016-01-18T00:00:00-08:00</updated><author><name></name></author><id>tag:www.randomlyunique.com,2016-01-18:data/2016/late-night-coding/</id><summary type="html">&lt;p&gt;What are commit messages like after normal working hours?  Using every public Github commit message from November 2015, I'll 
dive into this question with a simple natural language processing script! &lt;/p&gt;
&lt;p&gt;With the help of Apache Spark, I regexed out alphanumerics, removed stopwords, lemmatized, then binned unigrams
into daytime (9am to 9pm) and nighttime buckets. After this separation, 
I found the more common daytime(/nighttime) words by subtracting nighttime(/daytime) word frequencies from daytime(/nighttime) 
word frequencies. The word clouds below highlight these most common commit words: &lt;/p&gt;
&lt;p&gt;&lt;img alt="Photo" class="image_center_style" src="http://www.randomlyunique.com/data/2016/late-night-coding/late-night-coding.jpg" /&gt;&lt;/p&gt;</summary></entry></feed>